{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb7936b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T04:42:42.022577Z",
     "iopub.status.busy": "2023-05-14T04:42:42.021855Z",
     "iopub.status.idle": "2023-05-14T04:42:58.409148Z",
     "shell.execute_reply": "2023-05-14T04:42:58.407918Z"
    },
    "papermill": {
     "duration": 16.397745,
     "end_time": "2023-05-14T04:42:58.411728",
     "exception": false,
     "start_time": "2023-05-14T04:42:42.013983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pickle5\r\n",
      "  Downloading pickle5-0.0.11.tar.gz (132 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.1/132.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hBuilding wheels for collected packages: pickle5\r\n",
      "  Building wheel for pickle5 (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pickle5: filename=pickle5-0.0.11-cp310-cp310-linux_x86_64.whl size=125234 sha256=fa230fdb9fc1314a71c664fb1acbc0484d7ae499f8300e31b55a31e13b6553db\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/7d/14/ef/4aab19d27fa8e58772be5c71c16add0426acf9e1f64353235c\r\n",
      "Successfully built pickle5\r\n",
      "Installing collected packages: pickle5\r\n",
      "Successfully installed pickle5-0.0.11\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3475b98e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T04:42:58.423782Z",
     "iopub.status.busy": "2023-05-14T04:42:58.423463Z",
     "iopub.status.idle": "2023-05-14T04:43:05.036538Z",
     "shell.execute_reply": "2023-05-14T04:43:05.035422Z"
    },
    "papermill": {
     "duration": 6.622434,
     "end_time": "2023-05-14T04:43:05.039638",
     "exception": false,
     "start_time": "2023-05-14T04:42:58.417204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.getcwd())\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import re\n",
    "import locale\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle5 as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bef8f5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T04:43:05.056391Z",
     "iopub.status.busy": "2023-05-14T04:43:05.055661Z",
     "iopub.status.idle": "2023-05-14T04:43:05.072907Z",
     "shell.execute_reply": "2023-05-14T04:43:05.072136Z"
    },
    "papermill": {
     "duration": 0.027818,
     "end_time": "2023-05-14T04:43:05.075288",
     "exception": false,
     "start_time": "2023-05-14T04:43:05.047470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "locale.setlocale(locale.LC_ALL, '')\n",
    "\n",
    "_params = {}\n",
    "_param_aliases = {}\n",
    "def param(name, *args, **kwargs):\n",
    "    if name not in _params:\n",
    "        kwargs['name'] = name\n",
    "        param = tf.Variable(*args, **kwargs)\n",
    "        param.param = True\n",
    "        _params[name] = param\n",
    "    result = _params[name]\n",
    "    i = 0\n",
    "    while result in _param_aliases:\n",
    "        i += 1\n",
    "        result = _param_aliases[result]\n",
    "    return result\n",
    "\n",
    "def params_with_name(name):\n",
    "    return [p for n,p in _params.items() if name in n]\n",
    "\n",
    "def delete_all_params():\n",
    "    _params.clear()\n",
    "\n",
    "def alias_params(replace_dict):\n",
    "    for old,new in replace_dict.items():\n",
    "        _param_aliases[old] = new\n",
    "\n",
    "def delete_param_aliases():\n",
    "    _param_aliases.clear()\n",
    "\n",
    "def print_model_settings(locals_):\n",
    "    print(\"Uppercase local vars:\")\n",
    "    all_vars = [(k,v) for (k,v) in locals_.items() if (k.isupper() and k!='T' and k!='SETTINGS' and k!='ALL_SETTINGS')]\n",
    "    all_vars = sorted(all_vars, key=lambda x: x[0])\n",
    "    for var_name, var_value in all_vars:\n",
    "        print(\"\\t{}: {}\".format(var_name, var_value))\n",
    "\n",
    "\n",
    "def print_model_settings_dict(settings):\n",
    "    print(\"Settings dict:\")\n",
    "    all_vars = [(k,v) for (k,v) in settings.items()]\n",
    "    all_vars = sorted(all_vars, key=lambda x: x[0])\n",
    "    for var_name, var_value in all_vars:\n",
    "        print(\"\\t{}: {}\".format(var_name, var_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "209312d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T04:43:05.090825Z",
     "iopub.status.busy": "2023-05-14T04:43:05.090518Z",
     "iopub.status.idle": "2023-05-14T04:43:05.102991Z",
     "shell.execute_reply": "2023-05-14T04:43:05.102274Z"
    },
    "papermill": {
     "duration": 0.022615,
     "end_time": "2023-05-14T04:43:05.105344",
     "exception": false,
     "start_time": "2023-05-14T04:43:05.082729",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_since_beginning = collections.defaultdict(lambda: {})\n",
    "_since_last_flush = collections.defaultdict(lambda: {})\n",
    "\n",
    "_iter = [0]\n",
    "def tick():\n",
    "    _iter[0] += 1\n",
    "\n",
    "def plot(name, value):\n",
    "    _since_last_flush[name][_iter[0]] = value\n",
    "\n",
    "def flush():\n",
    "    prints = []\n",
    "\n",
    "    for name, vals in _since_last_flush.items():\n",
    "        prints.append(\"{}\\t{}\".format(name, np.mean(list(vals.values()))))\n",
    "        _since_beginning[name].update(vals)\n",
    "\n",
    "        x_vals = np.sort(list(_since_beginning[name].keys()))\n",
    "        y_vals = [_since_beginning[name][x] for x in x_vals]\n",
    "\n",
    "        plt.clf()\n",
    "        plt.plot(x_vals, y_vals)\n",
    "        plt.xlabel('iteration')\n",
    "        plt.ylabel(name)\n",
    "        plt.savefig(name.replace(' ', '_')+'.jpg')\n",
    "\n",
    "    print(\"iter {}\\t{}\".format(_iter[0], \"\\t\".join(prints)))\n",
    "    _since_last_flush.clear()\n",
    "\n",
    "    with open('log.pkl', 'wb') as f:\n",
    "        pickle.dump(dict(_since_beginning), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d094d43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T04:43:05.120915Z",
     "iopub.status.busy": "2023-05-14T04:43:05.120625Z",
     "iopub.status.idle": "2023-05-14T04:43:05.142159Z",
     "shell.execute_reply": "2023-05-14T04:43:05.141426Z"
    },
    "papermill": {
     "duration": 0.031813,
     "end_time": "2023-05-14T04:43:05.144506",
     "exception": false,
     "start_time": "2023-05-14T04:43:05.112693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_default_weightnorm = False\n",
    "def enable_default_weightnorm():\n",
    "    global _default_weightnorm\n",
    "    _default_weightnorm = True\n",
    "\n",
    "def Conv1D(name, input_dim, output_dim, filter_size, inputs, he_init=True, mask_type=None, stride=1, weightnorm=None, biases=True, gain=1.):\n",
    "    \"\"\"\n",
    "    inputs: tensor of shape (batch size, num channels, width)\n",
    "    mask_type: one of None, 'a', 'b'\n",
    "\n",
    "    returns: tensor of shape (batch size, num channels, width)\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name) as scope:\n",
    "\n",
    "        if mask_type is not None:\n",
    "            mask_type, mask_n_channels = mask_type\n",
    "\n",
    "            mask = np.ones(\n",
    "                (filter_size, input_dim, output_dim), \n",
    "                dtype='float32'\n",
    "            )\n",
    "            center = filter_size // 2\n",
    "\n",
    "            # Mask out future locations\n",
    "            # filter shape is (width, input channels, output channels)\n",
    "            mask[center+1:, :, :] = 0.\n",
    "\n",
    "            # Mask out future channels\n",
    "            for i in xrange(mask_n_channels):\n",
    "                for j in xrange(mask_n_channels):\n",
    "                    if (mask_type=='a' and i >= j) or (mask_type=='b' and i > j):\n",
    "                        mask[\n",
    "                            center,\n",
    "                            i::mask_n_channels,\n",
    "                            j::mask_n_channels\n",
    "                        ] = 0.\n",
    "\n",
    "\n",
    "        def uniform(stdev, size):\n",
    "            return np.random.uniform(\n",
    "                low=-stdev * np.sqrt(3),\n",
    "                high=stdev * np.sqrt(3),\n",
    "                size=size\n",
    "            ).astype('float32')\n",
    "\n",
    "        fan_in = input_dim * filter_size\n",
    "        fan_out = output_dim * filter_size / stride\n",
    "\n",
    "        if mask_type is not None: # only approximately correct\n",
    "            fan_in /= 2.\n",
    "            fan_out /= 2.\n",
    "\n",
    "        if he_init:\n",
    "            filters_stdev = np.sqrt(4./(fan_in+fan_out))\n",
    "        else: # Normalized init (Glorot & Bengio)\n",
    "            filters_stdev = np.sqrt(2./(fan_in+fan_out))\n",
    "\n",
    "        filter_values = uniform(\n",
    "            filters_stdev,\n",
    "            (filter_size, input_dim, output_dim)\n",
    "        )\n",
    "        filter_values *= gain\n",
    "\n",
    "        filters = param(name+'.Filters', filter_values)\n",
    "\n",
    "        if weightnorm==None:\n",
    "            weightnorm = _default_weightnorm\n",
    "        if weightnorm:\n",
    "            norm_values = np.sqrt(np.sum(np.square(filter_values), axis=(0,1)))\n",
    "            target_norms = param(\n",
    "                name + '.g',\n",
    "                norm_values\n",
    "            )\n",
    "            with tf.name_scope('weightnorm') as scope:\n",
    "                norms = tf.sqrt(tf.reduce_sum(tf.square(filters), reduction_indices=[0,1]))\n",
    "                filters = filters * (target_norms / norms)\n",
    "\n",
    "        if mask_type is not None:\n",
    "            with tf.name_scope('filter_mask'):\n",
    "                filters = filters * mask\n",
    "\n",
    "        result = tf.nn.conv1d(\n",
    "            inputs, \n",
    "            filters=filters, \n",
    "            stride=stride,\n",
    "            padding='SAME',\n",
    "            data_format='NCHW'\n",
    "        )\n",
    "\n",
    "        if biases:\n",
    "            _biases = param(\n",
    "                name+'.Biases',\n",
    "                np.zeros([output_dim], dtype='float32')\n",
    "            )\n",
    "\n",
    "            result = tf.expand_dims(result, 3)\n",
    "            result = tf.nn.bias_add(result, _biases, data_format='NCHW')\n",
    "            result = tf.squeeze(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaded84f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T04:43:05.160236Z",
     "iopub.status.busy": "2023-05-14T04:43:05.159929Z",
     "iopub.status.idle": "2023-05-14T04:43:05.186085Z",
     "shell.execute_reply": "2023-05-14T04:43:05.185365Z"
    },
    "papermill": {
     "duration": 0.036656,
     "end_time": "2023-05-14T04:43:05.188497",
     "exception": false,
     "start_time": "2023-05-14T04:43:05.151841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_default_weightnorm = False\n",
    "def enable_default_weightnorm():\n",
    "    global _default_weightnorm\n",
    "    _default_weightnorm = True\n",
    "\n",
    "def disable_default_weightnorm():\n",
    "    global _default_weightnorm\n",
    "    _default_weightnorm = False\n",
    "\n",
    "_weights_stdev = None\n",
    "def set_weights_stdev(weights_stdev):\n",
    "    global _weights_stdev\n",
    "    _weights_stdev = weights_stdev\n",
    "\n",
    "def unset_weights_stdev():\n",
    "    global _weights_stdev\n",
    "    _weights_stdev = None\n",
    "\n",
    "def Linear(\n",
    "        name, \n",
    "        input_dim, \n",
    "        output_dim, \n",
    "        inputs,\n",
    "        biases=True,\n",
    "        initialization=None,\n",
    "        weightnorm=None,\n",
    "        gain=1.\n",
    "        ):\n",
    "    \"\"\"\n",
    "    initialization: None, `lecun`, 'glorot', `he`, 'glorot_he', `orthogonal`, `(\"uniform\", range)`\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name) as scope:\n",
    "\n",
    "        def uniform(stdev, size):\n",
    "            if _weights_stdev is not None:\n",
    "                stdev = _weights_stdev\n",
    "            return np.random.uniform(\n",
    "                low=-stdev * np.sqrt(3),\n",
    "                high=stdev * np.sqrt(3),\n",
    "                size=size\n",
    "            ).astype('float32')\n",
    "\n",
    "        if initialization == 'lecun':# and input_dim != output_dim):\n",
    "            # disabling orth. init for now because it's too slow\n",
    "            weight_values = uniform(\n",
    "                np.sqrt(1./input_dim),\n",
    "                (input_dim, output_dim)\n",
    "            )\n",
    "\n",
    "        elif initialization == 'glorot' or (initialization == None):\n",
    "\n",
    "            weight_values = uniform(\n",
    "                np.sqrt(2./(input_dim+output_dim)),\n",
    "                (input_dim, output_dim)\n",
    "            )\n",
    "\n",
    "        elif initialization == 'he':\n",
    "\n",
    "            weight_values = uniform(\n",
    "                np.sqrt(2./input_dim),\n",
    "                (input_dim, output_dim)\n",
    "            )\n",
    "\n",
    "        elif initialization == 'glorot_he':\n",
    "\n",
    "            weight_values = uniform(\n",
    "                np.sqrt(4./(input_dim+output_dim)),\n",
    "                (input_dim, output_dim)\n",
    "            )\n",
    "\n",
    "        elif initialization == 'orthogonal' or \\\n",
    "            (initialization == None and input_dim == output_dim):\n",
    "            \n",
    "            # From lasagne\n",
    "            def sample(shape):\n",
    "                if len(shape) < 2:\n",
    "                    raise RuntimeError(\"Only shapes of length 2 or more are \"\n",
    "                                       \"supported.\")\n",
    "                flat_shape = (shape[0], np.prod(shape[1:]))\n",
    "                 # TODO: why normal and not uniform?\n",
    "                a = np.random.normal(0.0, 1.0, flat_shape)\n",
    "                u, _, v = np.linalg.svd(a, full_matrices=False)\n",
    "                # pick the one with the correct shape\n",
    "                q = u if u.shape == flat_shape else v\n",
    "                q = q.reshape(shape)\n",
    "                return q.astype('float32')\n",
    "            weight_values = sample((input_dim, output_dim))\n",
    "        \n",
    "        elif initialization[0] == 'uniform':\n",
    "        \n",
    "            weight_values = np.random.uniform(\n",
    "                low=-initialization[1],\n",
    "                high=initialization[1],\n",
    "                size=(input_dim, output_dim)\n",
    "            ).astype('float32')\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise Exception('Invalid initialization!')\n",
    "\n",
    "        weight_values *= gain\n",
    "\n",
    "        weight = param(\n",
    "            name + '.W',\n",
    "            weight_values\n",
    "        )\n",
    "\n",
    "        if weightnorm==None:\n",
    "            weightnorm = _default_weightnorm\n",
    "        if weightnorm:\n",
    "            norm_values = np.sqrt(np.sum(np.square(weight_values), axis=0))\n",
    "            # norm_values = np.linalg.norm(weight_values, axis=0)\n",
    "\n",
    "            target_norms = param(\n",
    "                name + '.g',\n",
    "                norm_values\n",
    "            )\n",
    "\n",
    "            with tf.name_scope('weightnorm') as scope:\n",
    "                norms = tf.sqrt(tf.reduce_sum(tf.square(weight), reduction_indices=[0]))\n",
    "                weight = weight * (target_norms / norms)\n",
    "\n",
    "        if inputs.get_shape().ndims == 2:\n",
    "            result = tf.matmul(inputs, weight)\n",
    "        else:\n",
    "            reshaped_inputs = tf.reshape(inputs, [-1, input_dim])\n",
    "            result = tf.matmul(reshaped_inputs, weight)\n",
    "            result = tf.reshape(result, tf.pack(tf.unpack(tf.shape(inputs))[:-1] + [output_dim]))\n",
    "\n",
    "        if biases:\n",
    "            result = tf.nn.bias_add(\n",
    "                result,\n",
    "                param(\n",
    "                    name + '.b',\n",
    "                    np.zeros((output_dim,), dtype='float32')\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0b5078d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T04:43:05.204424Z",
     "iopub.status.busy": "2023-05-14T04:43:05.204101Z",
     "iopub.status.idle": "2023-05-14T04:43:05.237150Z",
     "shell.execute_reply": "2023-05-14T04:43:05.236414Z"
    },
    "papermill": {
     "duration": 0.043573,
     "end_time": "2023-05-14T04:43:05.239560",
     "exception": false,
     "start_time": "2023-05-14T04:43:05.195987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_string(sample):\n",
    "    return tuple(sample.lower().split(' '))\n",
    "\n",
    "class NgramLanguageModel(object):\n",
    "    def __init__(self, n, samples, tokenize=False):\n",
    "        if tokenize:\n",
    "            tokenized_samples = []\n",
    "            for sample in samples:\n",
    "                tokenized_samples.append(tokenize_string(sample))\n",
    "            samples = tokenized_samples\n",
    "\n",
    "        self._n = n\n",
    "        self._samples = samples\n",
    "        self._ngram_counts = collections.defaultdict(int)\n",
    "        self._total_ngrams = 0\n",
    "        for ngram in self.ngrams():\n",
    "            self._ngram_counts[ngram] += 1\n",
    "            self._total_ngrams += 1\n",
    "\n",
    "    def ngrams(self):\n",
    "        n = self._n\n",
    "        for sample in self._samples:\n",
    "            for i in range(len(sample)-n+1):\n",
    "                yield sample[i:i+n]\n",
    "\n",
    "    def unique_ngrams(self):\n",
    "        return set(self._ngram_counts.keys())\n",
    "\n",
    "    def log_likelihood(self, ngram):\n",
    "        if ngram not in self._ngram_counts:\n",
    "            return -np.inf\n",
    "        else:\n",
    "            return np.log(self._ngram_counts[ngram]) - np.log(self._total_ngrams)\n",
    "\n",
    "    def kl_to(self, p):\n",
    "        # p is another NgramLanguageModel\n",
    "        log_likelihood_ratios = []\n",
    "        for ngram in p.ngrams():\n",
    "            log_likelihood_ratios.append(p.log_likelihood(ngram) - self.log_likelihood(ngram))\n",
    "        return np.mean(log_likelihood_ratios)\n",
    "\n",
    "    def cosine_sim_with(self, p):\n",
    "        # p is another NgramLanguageModel\n",
    "        p_dot_q = 0.\n",
    "        p_norm = 0.\n",
    "        q_norm = 0.\n",
    "        for ngram in p.unique_ngrams():\n",
    "            p_i = np.exp(p.log_likelihood(ngram))\n",
    "            q_i = np.exp(self.log_likelihood(ngram))\n",
    "            p_dot_q += p_i * q_i\n",
    "            p_norm += p_i**2\n",
    "        for ngram in self.unique_ngrams():\n",
    "            q_i = np.exp(self.log_likelihood(ngram))\n",
    "            q_norm += q_i**2\n",
    "        return p_dot_q / (np.sqrt(p_norm) * np.sqrt(q_norm))\n",
    "\n",
    "    def precision_wrt(self, p):\n",
    "        # p is another NgramLanguageModel\n",
    "        num = 0.\n",
    "        denom = 0\n",
    "        p_ngrams = p.unique_ngrams()\n",
    "        for ngram in self.unique_ngrams():\n",
    "            if ngram in p_ngrams:\n",
    "                num += self._ngram_counts[ngram]\n",
    "            denom += self._ngram_counts[ngram]\n",
    "        return float(num) / denom\n",
    "\n",
    "    def recall_wrt(self, p):\n",
    "        return p.precision_wrt(self)\n",
    "\n",
    "    def js_with(self, p):\n",
    "        log_p = np.array([p.log_likelihood(ngram) for ngram in p.unique_ngrams()])\n",
    "        log_q = np.array([self.log_likelihood(ngram) for ngram in p.unique_ngrams()])\n",
    "        log_m = np.logaddexp(log_p - np.log(2), log_q - np.log(2))\n",
    "        kl_p_m = np.sum(np.exp(log_p) * (log_p - log_m))\n",
    "\n",
    "        log_p = np.array([p.log_likelihood(ngram) for ngram in self.unique_ngrams()])\n",
    "        log_q = np.array([self.log_likelihood(ngram) for ngram in self.unique_ngrams()])\n",
    "        log_m = np.logaddexp(log_p - np.log(2), log_q - np.log(2))\n",
    "        kl_q_m = np.sum(np.exp(log_q) * (log_q - log_m))\n",
    "\n",
    "        return 0.5*(kl_p_m + kl_q_m) / np.log(2)\n",
    "\n",
    "def load_dataset(max_length, max_n_examples, tokenize=False, max_vocab_size=2048, data_dir=''):\n",
    "    print(\"loading dataset...\")\n",
    "    lines = []\n",
    "    finished = False\n",
    "\n",
    "    c = 0\n",
    "    with open(data_dir, 'r', encoding='latin1') as f:\n",
    "        for line in f:\n",
    "            c += 1\n",
    "            line = line[:-1]\n",
    "            if len(line) <= max_length:\n",
    "                lines.append(line + (\"`\" * (max_length-len(line))))\n",
    "\n",
    "    np.random.shuffle(lines)\n",
    "\n",
    "    counts = collections.Counter(char for line in lines for char in line)\n",
    "\n",
    "    charmap = {'unk':0}\n",
    "    inv_charmap = ['unk']\n",
    "\n",
    "    for char,count in counts.most_common(max_vocab_size-1):\n",
    "        if char not in charmap:\n",
    "            charmap[char] = len(inv_charmap)\n",
    "            inv_charmap.append(char)\n",
    "\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        filtered_line = []\n",
    "        for char in line:\n",
    "            if char in charmap:\n",
    "                filtered_line.append(char)\n",
    "            else:\n",
    "                filtered_line.append('unk')\n",
    "        filtered_lines.append(tuple(filtered_line))\n",
    "\n",
    "    for i in range(10):\n",
    "        print(filtered_lines[i])\n",
    "\n",
    "    print(\"loaded {} lines in dataset\".format(len(lines)))\n",
    "    return filtered_lines, charmap, inv_charmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d03710b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T04:43:05.255882Z",
     "iopub.status.busy": "2023-05-14T04:43:05.255577Z",
     "iopub.status.idle": "2023-05-14T04:43:39.655255Z",
     "shell.execute_reply": "2023-05-14T04:43:39.654290Z"
    },
    "papermill": {
     "duration": 34.415381,
     "end_time": "2023-05-14T04:43:39.662736",
     "exception": false,
     "start_time": "2023-05-14T04:43:05.247355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uppercase local vars:\n",
      "\tBATCH_SIZE: 64\n",
      "\tCRITIC_ITERS: 10\n",
      "\tDATA_DIR: /kaggle/input/newtrainandtestrockyou/train_rock_you.txt\n",
      "\tDIM: 128\n",
      "\tITERS: 200000\n",
      "\tLAMBDA: 10\n",
      "\tMAX_N_EXAMPLES: 10000000\n",
      "\tSEQ_LEN: 10\n",
      "loading dataset...\n",
      "('m', 'u', 'u', 'm', 'u', 'u', '9', '1', '5', '`')\n",
      "('r', 'e', 'd', 'p', 'o', 't', 'i', 'o', 'n', '`')\n",
      "('p', 'a', 'u', 'l', '2', '5', '8', '6', '`', '`')\n",
      "('C', 'r', 'e', 's', 't', '3', '2', '4', '`', '`')\n",
      "('b', 'e', 'x', 'a', 'n', 'd', 'h', 'a', 'n', '`')\n",
      "('k', 'e', 'o', 'p', 's', '1', '`', '`', '`', '`')\n",
      "('p', 'a', 'e', 'r', 'k', 'a', '`', '`', '`', '`')\n",
      "('t', 'e', 'w', 'a', 'r', 'a', 'k', 'i', '`', '`')\n",
      "('k', 'a', 't', 'e', '4', 'g', 'l', 'e', 'n', '`')\n",
      "('a', '8', '9', '4', '6', '7', '*', '`', '`', '`')\n",
      "loaded 7911499 lines in dataset\n"
     ]
    }
   ],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Download Google Billion Word at http://www.statmt.org/lm-benchmark/ and\n",
    "# fill in the path to the extracted files here!\n",
    "DATA_DIR = '/kaggle/input/newtrainandtestrockyou/train_rock_you.txt'\n",
    "if len(DATA_DIR) == 0:\n",
    "    raise Exception('Please specify path to data directory in gan_language.py!')\n",
    "\n",
    "BATCH_SIZE = 64 # Batch size\n",
    "ITERS = 200000 # How many iterations to train for\n",
    "SEQ_LEN = 10 # Sequence length in characters\n",
    "DIM = 128 # Model dimensionality. This is fairly slow and overfits, even on\n",
    "          # Billion Word. Consider decreasing for smaller datasets.\n",
    "CRITIC_ITERS = 10 # How many critic iterations per generator iteration. We\n",
    "                  # use 10 for the results in the paper, but 5 should work fine\n",
    "                  # as well.\n",
    "LAMBDA = 10 # Gradient penalty lambda hyperparameter.\n",
    "MAX_N_EXAMPLES = 10000000 # Max number of data examples to load. If data loading\n",
    "                          # is too slow or takes too much RAM, you can decrease\n",
    "                          # this (at the expense of having less training data).\n",
    "\n",
    "print_model_settings(locals().copy())\n",
    "\n",
    "lines, charmap, inv_charmap = load_dataset(\n",
    "    max_length=SEQ_LEN,\n",
    "    max_n_examples=MAX_N_EXAMPLES,\n",
    "    data_dir=DATA_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac590fe6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T04:43:39.675403Z",
     "iopub.status.busy": "2023-05-14T04:43:39.675095Z",
     "iopub.status.idle": "2023-05-14T04:43:39.686923Z",
     "shell.execute_reply": "2023-05-14T04:43:39.686068Z"
    },
    "papermill": {
     "duration": 0.020604,
     "end_time": "2023-05-14T04:43:39.689084",
     "exception": false,
     "start_time": "2023-05-14T04:43:39.668480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    return tf.reshape(\n",
    "        tf.nn.softmax(\n",
    "            tf.reshape(logits, [-1, len(charmap)])\n",
    "        ),\n",
    "        tf.shape(logits)\n",
    "    )\n",
    "\n",
    "def make_noise(shape):\n",
    "    return tf.random.normal(shape)\n",
    "\n",
    "def ResBlock(name, inputs):\n",
    "    output = inputs\n",
    "    output = tf.nn.relu(output)\n",
    "    output = Conv1D(name+'.1', DIM, DIM, 5, output)\n",
    "    output = tf.nn.relu(output)\n",
    "    output = Conv1D(name+'.2', DIM, DIM, 5, output)\n",
    "    return inputs + (0.3*output)\n",
    "\n",
    "def Generator(n_samples, prev_outputs=None):\n",
    "    output = make_noise(shape=[n_samples, 128])\n",
    "    output = Linear('Generator.Input', 128, SEQ_LEN*DIM, output)\n",
    "    output = tf.reshape(output, [-1, DIM, SEQ_LEN])\n",
    "    output = ResBlock('Generator.1', output)\n",
    "    output = ResBlock('Generator.2', output)\n",
    "    output = ResBlock('Generator.3', output)\n",
    "    output = ResBlock('Generator.4', output)\n",
    "    output = ResBlock('Generator.5', output)\n",
    "    output = Conv1D('Generator.Output', DIM, len(charmap), 1, output)\n",
    "    output = tf.transpose(output, [0, 2, 1])\n",
    "    output = softmax(output)\n",
    "    return output\n",
    "\n",
    "def Discriminator(inputs):\n",
    "    output = tf.transpose(inputs, [0,2,1])\n",
    "    output = Conv1D('Discriminator.Input', len(charmap), DIM, 1, output)\n",
    "    output = ResBlock('Discriminator.1', output)\n",
    "    output = ResBlock('Discriminator.2', output)\n",
    "    output = ResBlock('Discriminator.3', output)\n",
    "    output = ResBlock('Discriminator.4', output)\n",
    "    output = ResBlock('Discriminator.5', output)\n",
    "    output = tf.reshape(output, [-1, SEQ_LEN*DIM])\n",
    "    output = Linear('Discriminator.Output', SEQ_LEN*DIM, 1, output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c85a2de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T04:43:39.701580Z",
     "iopub.status.busy": "2023-05-14T04:43:39.701186Z",
     "iopub.status.idle": "2023-05-14T04:43:42.165705Z",
     "shell.execute_reply": "2023-05-14T04:43:42.164756Z"
    },
    "papermill": {
     "duration": 2.473374,
     "end_time": "2023-05-14T04:43:42.168015",
     "exception": false,
     "start_time": "2023-05-14T04:43:39.694641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "real_inputs_discrete = tf.compat.v1.placeholder(tf.int32, shape=[BATCH_SIZE, SEQ_LEN])\n",
    "real_inputs = tf.one_hot(real_inputs_discrete, len(charmap))\n",
    "fake_inputs = Generator(BATCH_SIZE)\n",
    "fake_inputs_discrete = tf.argmax(fake_inputs, fake_inputs.get_shape().ndims-1)\n",
    "\n",
    "disc_real = Discriminator(real_inputs)\n",
    "disc_fake = Discriminator(fake_inputs)\n",
    "\n",
    "disc_cost = tf.reduce_mean(disc_fake) - tf.reduce_mean(disc_real)\n",
    "gen_cost = -tf.reduce_mean(disc_fake)\n",
    "\n",
    "# WGAN lipschitz-penalty\n",
    "alpha = tf.random.uniform(\n",
    "    shape=[BATCH_SIZE,1,1], \n",
    "    minval=0.,\n",
    "    maxval=1.\n",
    ")\n",
    "differences = fake_inputs - real_inputs\n",
    "interpolates = real_inputs + (alpha*differences)\n",
    "gradients = tf.gradients(Discriminator(interpolates), [interpolates])[0]\n",
    "slopes = tf.sqrt(\n",
    "    tf.compat.v1.reduce_sum(\n",
    "        tf.square(gradients),\n",
    "        reduction_indices=[1,2]\n",
    "    )\n",
    ")\n",
    "gradient_penalty = tf.reduce_mean((slopes-1.)**2)\n",
    "disc_cost += LAMBDA*gradient_penalty\n",
    "\n",
    "gen_params = params_with_name('Generator')\n",
    "disc_params = params_with_name('Discriminator')\n",
    "\n",
    "gen_train_op = tf.compat.v1.train.AdamOptimizer(\n",
    "    learning_rate=1e-4,\n",
    "    beta1=0.5,\n",
    "    beta2=0.9\n",
    ").minimize(gen_cost, var_list=gen_params)\n",
    "disc_train_op = tf.compat.v1.train.AdamOptimizer(\n",
    "    learning_rate=1e-4,\n",
    "    beta1=0.5,\n",
    "    beta2=0.9\n",
    ").minimize(disc_cost, var_list=disc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b12cb13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T04:43:42.181318Z",
     "iopub.status.busy": "2023-05-14T04:43:42.181011Z",
     "iopub.status.idle": "2023-05-14T04:50:57.220649Z",
     "shell.execute_reply": "2023-05-14T04:50:57.219653Z"
    },
    "papermill": {
     "duration": 435.048921,
     "end_time": "2023-05-14T04:50:57.223114",
     "exception": false,
     "start_time": "2023-05-14T04:43:42.174193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation set JSD for n=1: 0.002756009875632112\n",
      "validation set JSD for n=2: 0.06803903681604705\n",
      "validation set JSD for n=3: 0.3561077785221999\n",
      "validation set JSD for n=4: 0.6899782819582897\n"
     ]
    }
   ],
   "source": [
    "# During training we monitor JS divergence between the true & generated ngram\n",
    "# distributions for n=1,2,3,4. To get an idea of the optimal values, we\n",
    "# evaluate these statistics on a held-out set first.\n",
    "true_char_ngram_lms = [NgramLanguageModel(i+1, lines[10*BATCH_SIZE:], tokenize=False) for i in range(4)]\n",
    "validation_char_ngram_lms = [NgramLanguageModel(i+1, lines[:10*BATCH_SIZE], tokenize=False) for i in range(4)]\n",
    "for i in range(4):\n",
    "    print(\"validation set JSD for n={}: {}\".format(i+1, true_char_ngram_lms[i].js_with(validation_char_ngram_lms[i])))\n",
    "true_char_ngram_lms = [NgramLanguageModel(i+1, lines, tokenize=False) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09dae100",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T04:50:57.237374Z",
     "iopub.status.busy": "2023-05-14T04:50:57.237055Z",
     "iopub.status.idle": "2023-05-14T04:50:57.242662Z",
     "shell.execute_reply": "2023-05-14T04:50:57.241709Z"
    },
    "papermill": {
     "duration": 0.014659,
     "end_time": "2023-05-14T04:50:57.244484",
     "exception": false,
     "start_time": "2023-05-14T04:50:57.229825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset iterator\n",
    "def inf_train_gen():\n",
    "    while True:\n",
    "        np.random.shuffle(lines)\n",
    "        for i in range(0, len(lines)-BATCH_SIZE+1, BATCH_SIZE):\n",
    "            yield np.array(\n",
    "                [[charmap[c] for c in l] for l in lines[i:i+BATCH_SIZE]], \n",
    "                dtype='int32'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2375c7fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-05-14T04:50:57.259221Z",
     "iopub.status.busy": "2023-05-14T04:50:57.257863Z",
     "iopub.status.idle": "2023-05-14T15:43:55.538311Z",
     "shell.execute_reply": "2023-05-14T15:43:55.537265Z"
    },
    "papermill": {
     "duration": 39178.29049,
     "end_time": "2023-05-14T15:43:55.541195",
     "exception": false,
     "start_time": "2023-05-14T04:50:57.250705",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 999\ttime\t0.186958220243454\ttrain disc cost\t-1.4172433614730835\tjs1\t0.11526546081138069\tjs2\t0.44174260161984436\tjs3\t0.6946261586981924\tjs4\t0.8387554223597862\n",
      "iter 1999\ttime\t0.1797353868484497\ttrain disc cost\t-0.9278169870376587\tjs1\t0.13299346220771366\tjs2\t0.36120152269032885\tjs3\t0.6204268458572783\tjs4\t0.8093802105719421\n",
      "iter 2999\ttime\t0.17964333581924438\ttrain disc cost\t-0.8690690994262695\tjs1\t0.10138845760824598\tjs2\t0.29952691347716726\tjs3\t0.5723015218349077\tjs4\t0.7908605905689322\n",
      "iter 3999\ttime\t0.17973480463027955\ttrain disc cost\t-0.7688076496124268\tjs1\t0.08863351060461155\tjs2\t0.2692688022707922\tjs3\t0.5474576418951405\tjs4\t0.7825664947999013\n",
      "iter 4999\ttime\t0.17919807720184328\ttrain disc cost\t-0.764358401298523\tjs1\t0.07198355727281186\tjs2\t0.2292270360095034\tjs3\t0.5028480472888389\tjs4\t0.7497098050815498\n",
      "iter 5999\ttime\t0.17955182814598084\ttrain disc cost\t-0.7272941470146179\tjs1\t0.07660838126643182\tjs2\t0.23193374027050276\tjs3\t0.5033067379485724\tjs4\t0.751190745985585\n",
      "iter 6999\ttime\t0.17985633277893068\ttrain disc cost\t-0.7430911064147949\tjs1\t0.05829884378260961\tjs2\t0.18827618947546387\tjs3\t0.4628374822342778\tjs4\t0.7428229446739785\n",
      "iter 7999\ttime\t0.17976323461532592\ttrain disc cost\t-0.7217279672622681\tjs1\t0.04771258405666287\tjs2\t0.16471789387427818\tjs3\t0.46372496975995714\tjs4\t0.7593502468534081\n",
      "iter 8999\ttime\t0.17934287905693055\ttrain disc cost\t-0.647122323513031\tjs1\t0.05177919617320321\tjs2\t0.1785898727487297\tjs3\t0.470015836754243\tjs4\t0.7512510425313441\n",
      "iter 9999\ttime\t0.17922439694404602\ttrain disc cost\t-0.6307576894760132\tjs1\t0.04538738977271092\tjs2\t0.153254310475345\tjs3\t0.44563638612346046\tjs4\t0.7421009545640039\n",
      "iter 10999\ttime\t0.1793958821296692\ttrain disc cost\t-0.6100562214851379\tjs1\t0.05305265337834226\tjs2\t0.1691882250859923\tjs3\t0.4456217750365878\tjs4\t0.7358764267575147\n",
      "iter 11999\ttime\t0.17930583000183106\ttrain disc cost\t-0.6144660711288452\tjs1\t0.053884363710170746\tjs2\t0.17539909912187795\tjs3\t0.4590459591012698\tjs4\t0.7436733046588427\n",
      "iter 12999\ttime\t0.18067805194854736\ttrain disc cost\t-0.6213070154190063\tjs1\t0.04468607941738123\tjs2\t0.15181968561920853\tjs3\t0.4404861966143642\tjs4\t0.7458168975683496\n",
      "iter 13999\ttime\t0.1792577874660492\ttrain disc cost\t-0.6214632391929626\tjs1\t0.03579468224202596\tjs2\t0.13487188946303022\tjs3\t0.432147226675216\tjs4\t0.7371579923243734\n",
      "iter 14999\ttime\t0.17961572170257567\ttrain disc cost\t-0.6304675340652466\tjs1\t0.038644022706486855\tjs2\t0.1330803628095027\tjs3\t0.42472655696919004\tjs4\t0.7358540963671953\n",
      "iter 15999\ttime\t0.17962181901931762\ttrain disc cost\t-0.6372918486595154\tjs1\t0.04078272767314784\tjs2\t0.13826086644238628\tjs3\t0.430123626031079\tjs4\t0.7410748742189817\n",
      "iter 16999\ttime\t0.17962023091316223\ttrain disc cost\t-0.6318124532699585\tjs1\t0.04270869989694774\tjs2\t0.1439423656470779\tjs3\t0.44456405808539057\tjs4\t0.7486784450001587\n",
      "iter 17999\ttime\t0.17973697280883788\ttrain disc cost\t-0.6392407417297363\tjs1\t0.0377466398555883\tjs2\t0.12964215653842742\tjs3\t0.4197365754047423\tjs4\t0.726126917527577\n",
      "iter 18999\ttime\t0.1794556438922882\ttrain disc cost\t-0.6306822896003723\tjs1\t0.03762169203612217\tjs2\t0.1261488427630711\tjs3\t0.42539477062287373\tjs4\t0.7485981575402573\n",
      "iter 19999\ttime\t0.1792996346950531\ttrain disc cost\t-0.6445091366767883\tjs1\t0.038913044258470586\tjs2\t0.1287457927033786\tjs3\t0.42302564316566443\tjs4\t0.7363220452621982\n",
      "iter 20999\ttime\t0.17926585507392884\ttrain disc cost\t-0.6426853537559509\tjs1\t0.041720222319078286\tjs2\t0.13160894242641283\tjs3\t0.4215134257393716\tjs4\t0.7334499092934976\n",
      "iter 21999\ttime\t0.17946640300750732\ttrain disc cost\t-0.6489539742469788\tjs1\t0.0402948029111599\tjs2\t0.13077989602120038\tjs3\t0.4268363877120123\tjs4\t0.7456310465620315\n",
      "iter 22999\ttime\t0.17948332142829895\ttrain disc cost\t-0.6293832063674927\tjs1\t0.038531817174428065\tjs2\t0.12768711833819632\tjs3\t0.4162141418690763\tjs4\t0.7293865043829032\n",
      "iter 23999\ttime\t0.17925383758544922\ttrain disc cost\t-0.6394342184066772\tjs1\t0.04378504492927749\tjs2\t0.13619194494799486\tjs3\t0.43424318596523276\tjs4\t0.744650048159827\n",
      "iter 24999\ttime\t0.180819171667099\ttrain disc cost\t-0.6359359622001648\tjs1\t0.03494975201341893\tjs2\t0.118006780463372\tjs3\t0.40406343819536394\tjs4\t0.7163862339095468\n",
      "iter 25999\ttime\t0.17929412961006164\ttrain disc cost\t-0.6451191306114197\tjs1\t0.03703394720741908\tjs2\t0.12213829561277245\tjs3\t0.40582700091889445\tjs4\t0.7170617030973807\n",
      "iter 26999\ttime\t0.179434814453125\ttrain disc cost\t-0.6535794734954834\tjs1\t0.03800118410248484\tjs2\t0.12166159979178268\tjs3\t0.4091284360465677\tjs4\t0.7268956655918134\n",
      "iter 27999\ttime\t0.17944576382637023\ttrain disc cost\t-0.6528539061546326\tjs1\t0.03721595321328156\tjs2\t0.12581161850402464\tjs3\t0.40968050991144817\tjs4\t0.7159803638782537\n",
      "iter 28999\ttime\t0.17945573377609253\ttrain disc cost\t-0.6519073247909546\tjs1\t0.03833697986832425\tjs2\t0.12548936121947618\tjs3\t0.4102845315562297\tjs4\t0.7237644195603157\n",
      "iter 29999\ttime\t0.17942775845527648\ttrain disc cost\t-0.6433447003364563\tjs1\t0.039115448258136266\tjs2\t0.12574331992252183\tjs3\t0.41795657702699013\tjs4\t0.7243508366332451\n",
      "iter 30999\ttime\t0.17954316878318788\ttrain disc cost\t-0.6472653746604919\tjs1\t0.03600580313273325\tjs2\t0.12076147396554238\tjs3\t0.40390821919457437\tjs4\t0.7204396784331417\n",
      "iter 31999\ttime\t0.17935487961769103\ttrain disc cost\t-0.6522026658058167\tjs1\t0.038082061617332984\tjs2\t0.12733522667561803\tjs3\t0.41534326846902325\tjs4\t0.7210851070314614\n",
      "iter 32999\ttime\t0.17902810478210449\ttrain disc cost\t-0.6460753679275513\tjs1\t0.034617380677807806\tjs2\t0.12189363328344528\tjs3\t0.4170931161739912\tjs4\t0.7287198062234745\n",
      "iter 33999\ttime\t0.17946119809150696\ttrain disc cost\t-0.6606292724609375\tjs1\t0.04126022678498871\tjs2\t0.129566517538932\tjs3\t0.4139630548616729\tjs4\t0.7263503781676118\n",
      "iter 34999\ttime\t0.17951246571540833\ttrain disc cost\t-0.6508640050888062\tjs1\t0.0340781451885745\tjs2\t0.12376571304765208\tjs3\t0.41383356734496146\tjs4\t0.7280389719377605\n",
      "iter 35999\ttime\t0.17917617726325988\ttrain disc cost\t-0.6504683494567871\tjs1\t0.03700768170945772\tjs2\t0.12020319385533872\tjs3\t0.40850778092737206\tjs4\t0.7254993315314139\n",
      "iter 36999\ttime\t0.17910908317565918\ttrain disc cost\t-0.6408738493919373\tjs1\t0.037272749550702866\tjs2\t0.13064696138576928\tjs3\t0.4122614297953582\tjs4\t0.7269413696990654\n",
      "iter 37999\ttime\t0.18054421257972716\ttrain disc cost\t-0.6396820545196533\tjs1\t0.0340870918132117\tjs2\t0.11568453944832702\tjs3\t0.401586968187056\tjs4\t0.7214948767257141\n",
      "iter 38999\ttime\t0.17934409928321837\ttrain disc cost\t-0.6473898887634277\tjs1\t0.03235807472514769\tjs2\t0.11262396597178073\tjs3\t0.3974086526881529\tjs4\t0.7183717787293377\n",
      "iter 39999\ttime\t0.1795555067062378\ttrain disc cost\t-0.6381357908248901\tjs1\t0.03651036331927856\tjs2\t0.11519091371236004\tjs3\t0.39014482185329247\tjs4\t0.710826839640255\n",
      "iter 40999\ttime\t0.17926785445213317\ttrain disc cost\t-0.6384316682815552\tjs1\t0.039394866183010274\tjs2\t0.12372871022200108\tjs3\t0.4124938147612005\tjs4\t0.727078425162222\n",
      "iter 41999\ttime\t0.17972101211547853\ttrain disc cost\t-0.6441253423690796\tjs1\t0.03303501745661448\tjs2\t0.11211443300661805\tjs3\t0.39947331023111654\tjs4\t0.7219699861111545\n",
      "iter 42999\ttime\t0.18014849877357483\ttrain disc cost\t-0.6410126686096191\tjs1\t0.038727134030946596\tjs2\t0.12476044669568247\tjs3\t0.40834804746277437\tjs4\t0.7186327828960763\n",
      "iter 43999\ttime\t0.1804635033607483\ttrain disc cost\t-0.6580601930618286\tjs1\t0.034750866029505674\tjs2\t0.1157515359011438\tjs3\t0.3966453821181204\tjs4\t0.7142246355655397\n",
      "iter 44999\ttime\t0.17955581092834472\ttrain disc cost\t-0.6395007371902466\tjs1\t0.03208416033446414\tjs2\t0.10722239367885048\tjs3\t0.391773611725491\tjs4\t0.7215368996528071\n",
      "iter 45999\ttime\t0.17937293243408203\ttrain disc cost\t-0.6494922041893005\tjs1\t0.03603856739358102\tjs2\t0.12369334615025714\tjs3\t0.4078111149640481\tjs4\t0.7280436247747281\n",
      "iter 46999\ttime\t0.18006020784378052\ttrain disc cost\t-0.6383937001228333\tjs1\t0.03191848742436129\tjs2\t0.11282758123127613\tjs3\t0.40128462810146787\tjs4\t0.7226069102903712\n",
      "iter 47999\ttime\t0.1799875283241272\ttrain disc cost\t-0.6414724588394165\tjs1\t0.034475042031443894\tjs2\t0.11731247644340768\tjs3\t0.3980577075143447\tjs4\t0.7124742404822686\n",
      "iter 48999\ttime\t0.179661052942276\ttrain disc cost\t-0.6363270282745361\tjs1\t0.03191625584319817\tjs2\t0.11262477899836529\tjs3\t0.39791980273014027\tjs4\t0.7137415616242067\n",
      "iter 49999\ttime\t0.18113500213623046\ttrain disc cost\t-0.6408493518829346\tjs1\t0.03055916546525906\tjs2\t0.10917870091762108\tjs3\t0.4000312462320575\tjs4\t0.7248110521676763\n",
      "iter 50999\ttime\t0.17941388249397278\ttrain disc cost\t-0.6411007046699524\tjs1\t0.03648129250149771\tjs2\t0.11744320249725101\tjs3\t0.4026821030186605\tjs4\t0.720034945919072\n",
      "iter 51999\ttime\t0.17972041845321654\ttrain disc cost\t-0.637641966342926\tjs1\t0.04036178363772791\tjs2\t0.1338198536300915\tjs3\t0.42554563749181373\tjs4\t0.7456610599159462\n",
      "iter 52999\ttime\t0.17968078804016113\ttrain disc cost\t-0.6317676901817322\tjs1\t0.03195215734149544\tjs2\t0.11786317027336247\tjs3\t0.401883405275729\tjs4\t0.724741357509921\n",
      "iter 53999\ttime\t0.179362238407135\ttrain disc cost\t-0.6346502900123596\tjs1\t0.029470708436421272\tjs2\t0.10868130814705342\tjs3\t0.3942359042940246\tjs4\t0.7149145508369954\n",
      "iter 54999\ttime\t0.17926093649864197\ttrain disc cost\t-0.6294918656349182\tjs1\t0.03262024082981304\tjs2\t0.12124130877136903\tjs3\t0.40935281920616673\tjs4\t0.7282031798753058\n",
      "iter 55999\ttime\t0.179226989030838\ttrain disc cost\t-0.6228876709938049\tjs1\t0.0377235305038038\tjs2\t0.12714564471163872\tjs3\t0.4259192091069087\tjs4\t0.7355768370949975\n",
      "iter 56999\ttime\t0.17917045736312867\ttrain disc cost\t-0.6278511881828308\tjs1\t0.03435406777431507\tjs2\t0.1210389743757963\tjs3\t0.4101340796476017\tjs4\t0.7319151519607943\n",
      "iter 57999\ttime\t0.17922447752952575\ttrain disc cost\t-0.6253374218940735\tjs1\t0.034815504719416204\tjs2\t0.12171929950148933\tjs3\t0.41349828326821536\tjs4\t0.7308319477276729\n",
      "iter 58999\ttime\t0.17916058659553527\ttrain disc cost\t-0.6275873184204102\tjs1\t0.031726809036348946\tjs2\t0.11822269526988945\tjs3\t0.4008059277186588\tjs4\t0.7200082566073318\n",
      "iter 59999\ttime\t0.17930237531661988\ttrain disc cost\t-0.6306920051574707\tjs1\t0.032737736582812586\tjs2\t0.11813964251498639\tjs3\t0.39139354522940295\tjs4\t0.7114913038148474\n",
      "iter 60999\ttime\t0.17903356432914733\ttrain disc cost\t-0.6324916481971741\tjs1\t0.037250056996892586\tjs2\t0.12250109800703407\tjs3\t0.39403224239572543\tjs4\t0.7127296678401289\n",
      "iter 61999\ttime\t0.18039768528938294\ttrain disc cost\t-0.6247652769088745\tjs1\t0.033103384443588194\tjs2\t0.11664358098066784\tjs3\t0.4136926444983439\tjs4\t0.7396860422706643\n",
      "iter 62999\ttime\t0.17928995060920716\ttrain disc cost\t-0.622389554977417\tjs1\t0.032365852349875726\tjs2\t0.11556507114953707\tjs3\t0.40341634465467574\tjs4\t0.7255802894896541\n",
      "iter 63999\ttime\t0.1792248318195343\ttrain disc cost\t-0.6141601204872131\tjs1\t0.0289570580564118\tjs2\t0.10910566094980885\tjs3\t0.387956152026901\tjs4\t0.7097424009003884\n",
      "iter 64999\ttime\t0.1791504764556885\ttrain disc cost\t-0.5691797137260437\tjs1\t0.029258886171051966\tjs2\t0.11179669716169277\tjs3\t0.4058539108029508\tjs4\t0.7257895488779039\n",
      "iter 65999\ttime\t0.17920652198791504\ttrain disc cost\t-0.5749417543411255\tjs1\t0.030957260371023206\tjs2\t0.11026000176802114\tjs3\t0.4049620361775331\tjs4\t0.7310239710686185\n",
      "iter 66999\ttime\t0.179049724817276\ttrain disc cost\t-0.575897753238678\tjs1\t0.02851041687928113\tjs2\t0.11207674171209862\tjs3\t0.41069019012245805\tjs4\t0.7322362718445384\n",
      "iter 67999\ttime\t0.17940355086326598\ttrain disc cost\t-0.5727300047874451\tjs1\t0.029335209219591936\tjs2\t0.10769256078164226\tjs3\t0.39533019257783747\tjs4\t0.7219564131766399\n",
      "iter 68999\ttime\t0.17939598798751832\ttrain disc cost\t-0.5672376155853271\tjs1\t0.030342276192392537\tjs2\t0.10983984276682528\tjs3\t0.4023182087589038\tjs4\t0.7242056344486042\n",
      "iter 69999\ttime\t0.17910122871398926\ttrain disc cost\t-0.5831251740455627\tjs1\t0.02921514980168996\tjs2\t0.1168958113891603\tjs3\t0.40690769438607155\tjs4\t0.7313553187941052\n",
      "iter 70999\ttime\t0.179307936668396\ttrain disc cost\t-0.5716705918312073\tjs1\t0.026962115442908698\tjs2\t0.10990675915013681\tjs3\t0.39065612476866657\tjs4\t0.7184612486916455\n",
      "iter 71999\ttime\t0.17940111780166626\ttrain disc cost\t-0.567794144153595\tjs1\t0.029849846865535903\tjs2\t0.11348051953109527\tjs3\t0.4083123321316504\tjs4\t0.7287424702425167\n",
      "iter 72999\ttime\t0.1794911639690399\ttrain disc cost\t-0.5710053443908691\tjs1\t0.02691588474401019\tjs2\t0.11013711316295355\tjs3\t0.40410096066385687\tjs4\t0.7299820749404744\n",
      "iter 73999\ttime\t0.17929491090774535\ttrain disc cost\t-0.5727075338363647\tjs1\t0.030615468038203444\tjs2\t0.11493796395451296\tjs3\t0.397581393840786\tjs4\t0.7161653337942452\n",
      "iter 74999\ttime\t0.18053239679336547\ttrain disc cost\t-0.5688070058822632\tjs1\t0.02824123377072251\tjs2\t0.11521297236045304\tjs3\t0.406030056864314\tjs4\t0.7240240369744887\n",
      "iter 75999\ttime\t0.17925684905052186\ttrain disc cost\t-0.5718104839324951\tjs1\t0.03278313447434233\tjs2\t0.12094608464300578\tjs3\t0.40370787560403926\tjs4\t0.7209501175139887\n",
      "iter 76999\ttime\t0.1793077254295349\ttrain disc cost\t-0.5713351964950562\tjs1\t0.028684854685934024\tjs2\t0.10939359634645991\tjs3\t0.39638772162663993\tjs4\t0.7192535684810862\n",
      "iter 77999\ttime\t0.17935617542266846\ttrain disc cost\t-0.5681599378585815\tjs1\t0.028518769089849804\tjs2\t0.10927727163562119\tjs3\t0.3977613666917424\tjs4\t0.719888801663328\n",
      "iter 78999\ttime\t0.17917690324783325\ttrain disc cost\t-0.5736074447631836\tjs1\t0.02983853833063047\tjs2\t0.11251592912693203\tjs3\t0.4055872720665791\tjs4\t0.7271842190419728\n",
      "iter 79999\ttime\t0.17923655796051025\ttrain disc cost\t-0.57053542137146\tjs1\t0.029278911951305414\tjs2\t0.10688935105557075\tjs3\t0.38592670397937245\tjs4\t0.7112735557341081\n",
      "iter 80999\ttime\t0.17921532416343688\ttrain disc cost\t-0.5709167718887329\tjs1\t0.028310620077491695\tjs2\t0.1119110977177041\tjs3\t0.4010271938751048\tjs4\t0.7271927924819569\n",
      "iter 81999\ttime\t0.17901980519294738\ttrain disc cost\t-0.5686765313148499\tjs1\t0.02733189785728608\tjs2\t0.10388554230799497\tjs3\t0.3794046944823645\tjs4\t0.7019517949972138\n",
      "iter 82999\ttime\t0.17941293144226075\ttrain disc cost\t-0.572417140007019\tjs1\t0.028236389454786674\tjs2\t0.10902764962293413\tjs3\t0.40376839688265015\tjs4\t0.719308737904555\n",
      "iter 83999\ttime\t0.1792464590072632\ttrain disc cost\t-0.5647566914558411\tjs1\t0.027629013813499435\tjs2\t0.10806345249270648\tjs3\t0.39128556878410914\tjs4\t0.7086356679522573\n",
      "iter 84999\ttime\t0.17917076182365418\ttrain disc cost\t-0.5632829666137695\tjs1\t0.029141562354262743\tjs2\t0.1088505064318297\tjs3\t0.39667165536719173\tjs4\t0.7161342333956722\n",
      "iter 85999\ttime\t0.17937769699096678\ttrain disc cost\t-0.5626658797264099\tjs1\t0.029566236980149378\tjs2\t0.10971524598505197\tjs3\t0.4016016409792087\tjs4\t0.7165792939608385\n",
      "iter 86999\ttime\t0.18067404222488403\ttrain disc cost\t-0.5756019353866577\tjs1\t0.027514082173895338\tjs2\t0.10922183080855706\tjs3\t0.3938438865277414\tjs4\t0.724339649892767\n",
      "iter 87999\ttime\t0.17902714037895204\ttrain disc cost\t-0.5651736855506897\tjs1\t0.02631769340653442\tjs2\t0.10478918073218502\tjs3\t0.39914245386703395\tjs4\t0.7239522033494199\n",
      "iter 88999\ttime\t0.17919628858566283\ttrain disc cost\t-0.5674228072166443\tjs1\t0.02959924210733273\tjs2\t0.10757361051315283\tjs3\t0.39110925878692626\tjs4\t0.7097110410476904\n",
      "iter 89999\ttime\t0.17922856163978576\ttrain disc cost\t-0.5687010288238525\tjs1\t0.032966844032756897\tjs2\t0.1212230383396209\tjs3\t0.41177444114024053\tjs4\t0.7230507466524888\n",
      "iter 90999\ttime\t0.17914778423309327\ttrain disc cost\t-0.5712138414382935\tjs1\t0.027903701305559993\tjs2\t0.109591781616364\tjs3\t0.4031146941486469\tjs4\t0.7256666576905695\n",
      "iter 91999\ttime\t0.17919365310668944\ttrain disc cost\t-0.5677521824836731\tjs1\t0.026059674577414857\tjs2\t0.10724244950916119\tjs3\t0.3853859622522503\tjs4\t0.7077656440008622\n",
      "iter 92999\ttime\t0.179006046295166\ttrain disc cost\t-0.5757772326469421\tjs1\t0.03109160109937533\tjs2\t0.11553788458967322\tjs3\t0.40393182299068864\tjs4\t0.7212394520865387\n",
      "iter 93999\ttime\t0.17910866379737855\ttrain disc cost\t-0.5785115957260132\tjs1\t0.027679797403771998\tjs2\t0.10846151390585095\tjs3\t0.4017603503144201\tjs4\t0.7239114666815669\n",
      "iter 94999\ttime\t0.17933181524276734\ttrain disc cost\t-0.5723274350166321\tjs1\t0.026313512787517094\tjs2\t0.11045018518016551\tjs3\t0.3936064236356058\tjs4\t0.7116218151430184\n",
      "iter 95999\ttime\t0.1791898157596588\ttrain disc cost\t-0.570899248123169\tjs1\t0.030512015024161232\tjs2\t0.11593316018296412\tjs3\t0.40069975057236545\tjs4\t0.7234808194670646\n",
      "iter 96999\ttime\t0.1791611008644104\ttrain disc cost\t-0.5665961503982544\tjs1\t0.02559080692588613\tjs2\t0.10600211000409075\tjs3\t0.4005653026163912\tjs4\t0.7248020961765121\n",
      "iter 97999\ttime\t0.17897186732292175\ttrain disc cost\t-0.5696806907653809\tjs1\t0.030307225918165075\tjs2\t0.11484069668721791\tjs3\t0.3917417932245526\tjs4\t0.7204274017831991\n",
      "iter 98999\ttime\t0.18045678544044494\ttrain disc cost\t-0.5739973187446594\tjs1\t0.03072632988446017\tjs2\t0.11254931673191136\tjs3\t0.39742076699282997\tjs4\t0.7094857873078249\n",
      "iter 99999\ttime\t0.17918366003036498\ttrain disc cost\t-0.5705404281616211\tjs1\t0.028270893519186994\tjs2\t0.10670550746767368\tjs3\t0.3900388419251646\tjs4\t0.7154298122168087\n",
      "iter 100999\ttime\t0.17924063467979431\ttrain disc cost\t-0.5585647821426392\tjs1\t0.028640843292168438\tjs2\t0.11211530425384625\tjs3\t0.3975500097363154\tjs4\t0.7242088579776577\n",
      "iter 101999\ttime\t0.17941058301925658\ttrain disc cost\t-0.5665661692619324\tjs1\t0.028451358810927283\tjs2\t0.10479663270535258\tjs3\t0.393987809192049\tjs4\t0.7180329622552434\n",
      "iter 102999\ttime\t0.17910651278495787\ttrain disc cost\t-0.5642510056495667\tjs1\t0.03671191145499198\tjs2\t0.12619954723518975\tjs3\t0.42035392708842734\tjs4\t0.7335763376772774\n",
      "iter 103999\ttime\t0.17927558875083924\ttrain disc cost\t-0.5660755634307861\tjs1\t0.028313501348498808\tjs2\t0.11176062022996826\tjs3\t0.3978359398736654\tjs4\t0.7137892618518038\n",
      "iter 104999\ttime\t0.17933829379081725\ttrain disc cost\t-0.5667357444763184\tjs1\t0.03094324175690457\tjs2\t0.11454268580806373\tjs3\t0.39584331977282067\tjs4\t0.7158832655436866\n",
      "iter 105999\ttime\t0.17922928547859193\ttrain disc cost\t-0.5677130818367004\tjs1\t0.02822076587643693\tjs2\t0.10648996860276788\tjs3\t0.39665416500856654\tjs4\t0.7199122461817324\n",
      "iter 106999\ttime\t0.17924635076522827\ttrain disc cost\t-0.5648278594017029\tjs1\t0.026369187799445817\tjs2\t0.10533519571223207\tjs3\t0.38919245105138733\tjs4\t0.709895918423234\n",
      "iter 107999\ttime\t0.17936000633239746\ttrain disc cost\t-0.5615703463554382\tjs1\t0.026721620353862562\tjs2\t0.10290019574405261\tjs3\t0.3901079613377806\tjs4\t0.7169226765282863\n",
      "iter 108999\ttime\t0.17913716506958008\ttrain disc cost\t-0.5524049997329712\tjs1\t0.02707574715890888\tjs2\t0.1123439939901092\tjs3\t0.39484839059867105\tjs4\t0.719151039978441\n",
      "iter 109999\ttime\t0.17940807127952577\ttrain disc cost\t-0.5471856594085693\tjs1\t0.026766452934957677\tjs2\t0.10879202899582019\tjs3\t0.39512830051825537\tjs4\t0.7192469589009817\n",
      "iter 110999\ttime\t0.1792244517803192\ttrain disc cost\t-0.5500666499137878\tjs1\t0.029534495934892647\tjs2\t0.11458469509363634\tjs3\t0.40885556221236835\tjs4\t0.7310185919048516\n",
      "iter 111999\ttime\t0.18072927260398866\ttrain disc cost\t-0.5582737922668457\tjs1\t0.02561468979105286\tjs2\t0.10964877345647525\tjs3\t0.38861992869454987\tjs4\t0.715963031533829\n",
      "iter 112999\ttime\t0.1792482976913452\ttrain disc cost\t-0.5492391586303711\tjs1\t0.02438108135059274\tjs2\t0.10290392498332421\tjs3\t0.39842940224306006\tjs4\t0.7317169242071547\n",
      "iter 113999\ttime\t0.17917466831207277\ttrain disc cost\t-0.5517617464065552\tjs1\t0.02962785370627049\tjs2\t0.11094805454779541\tjs3\t0.4026241347728011\tjs4\t0.7239572576448658\n",
      "iter 114999\ttime\t0.17935117626190186\ttrain disc cost\t-0.5488350987434387\tjs1\t0.024772870952143877\tjs2\t0.1077629675726815\tjs3\t0.38806181350409924\tjs4\t0.7171135153926299\n",
      "iter 115999\ttime\t0.17932821202278137\ttrain disc cost\t-0.5462659597396851\tjs1\t0.029690177335246266\tjs2\t0.10519843007383636\tjs3\t0.3965909824433032\tjs4\t0.7195659640237204\n",
      "iter 116999\ttime\t0.17940165543556214\ttrain disc cost\t-0.5585113167762756\tjs1\t0.026062142456221744\tjs2\t0.10882465352560919\tjs3\t0.3888915099532597\tjs4\t0.7117297191587029\n",
      "iter 117999\ttime\t0.17936088681221007\ttrain disc cost\t-0.5567871332168579\tjs1\t0.030250795513033585\tjs2\t0.11137533065962449\tjs3\t0.39936494082920565\tjs4\t0.722727713716985\n",
      "iter 118999\ttime\t0.17920317625999452\ttrain disc cost\t-0.5651527047157288\tjs1\t0.026234996270208606\tjs2\t0.10475597632839664\tjs3\t0.3895027743165164\tjs4\t0.7121947605475266\n",
      "iter 119999\ttime\t0.1792458701133728\ttrain disc cost\t-0.5580102205276489\tjs1\t0.026561387729376648\tjs2\t0.10654800355505444\tjs3\t0.3893883901201386\tjs4\t0.7055977214219992\n",
      "iter 120999\ttime\t0.17951483273506164\ttrain disc cost\t-0.5576439499855042\tjs1\t0.02850216968796306\tjs2\t0.1080398877318016\tjs3\t0.39276458424154354\tjs4\t0.7200413616993424\n",
      "iter 121999\ttime\t0.1793264615535736\ttrain disc cost\t-0.5570603013038635\tjs1\t0.0268917881980288\tjs2\t0.11014312613409\tjs3\t0.40327346066529984\tjs4\t0.7242301830150738\n",
      "iter 122999\ttime\t0.17935389971733093\ttrain disc cost\t-0.5463650226593018\tjs1\t0.028076562674653832\tjs2\t0.1101499295624646\tjs3\t0.4003310637281041\tjs4\t0.7252055164676552\n",
      "iter 123999\ttime\t0.1805645945072174\ttrain disc cost\t-0.5446563363075256\tjs1\t0.03343120793174957\tjs2\t0.12317509949863421\tjs3\t0.4198710188183964\tjs4\t0.7317455889934672\n",
      "iter 124999\ttime\t0.17963449168205262\ttrain disc cost\t-0.5427210927009583\tjs1\t0.027396334144351695\tjs2\t0.10756194251485739\tjs3\t0.38968807719432075\tjs4\t0.7051229062363216\n",
      "iter 125999\ttime\t0.17966594219207763\ttrain disc cost\t-0.5366176962852478\tjs1\t0.023698873814861767\tjs2\t0.10593689491556717\tjs3\t0.39568293558240236\tjs4\t0.7148795200339825\n",
      "iter 126999\ttime\t0.18017699432373047\ttrain disc cost\t-0.504697322845459\tjs1\t0.023091596607418642\tjs2\t0.11172348132924852\tjs3\t0.40630414430152195\tjs4\t0.7356379843732468\n",
      "iter 127999\ttime\t0.18020191001892089\ttrain disc cost\t-0.5062969923019409\tjs1\t0.021737340797118573\tjs2\t0.10246552710118399\tjs3\t0.3938105010945769\tjs4\t0.7109443076561485\n",
      "iter 128999\ttime\t0.180028080701828\ttrain disc cost\t-0.4986027181148529\tjs1\t0.02432139625566817\tjs2\t0.10210531446184674\tjs3\t0.38349555296283344\tjs4\t0.7109550855037369\n",
      "iter 129999\ttime\t0.18027011346817018\ttrain disc cost\t-0.4983992874622345\tjs1\t0.021600071060401523\tjs2\t0.10681037925333628\tjs3\t0.38647602271252385\tjs4\t0.7112072442508375\n",
      "iter 130999\ttime\t0.18017189574241638\ttrain disc cost\t-0.4915968179702759\tjs1\t0.024404024497066384\tjs2\t0.11390985462696833\tjs3\t0.41033372576990795\tjs4\t0.7237889302220408\n",
      "iter 131999\ttime\t0.1801313455104828\ttrain disc cost\t-0.4908824861049652\tjs1\t0.020815221256179614\tjs2\t0.10351461857296351\tjs3\t0.3886926729495195\tjs4\t0.7215557659431566\n",
      "iter 132999\ttime\t0.180154141664505\ttrain disc cost\t-0.4897817075252533\tjs1\t0.023143821357705114\tjs2\t0.10684264774386287\tjs3\t0.3980688479386101\tjs4\t0.7217805635430783\n",
      "iter 133999\ttime\t0.180200439453125\ttrain disc cost\t-0.49534350633621216\tjs1\t0.021259160151398813\tjs2\t0.10485692808174171\tjs3\t0.3928847463019502\tjs4\t0.727112404925703\n",
      "iter 134999\ttime\t0.1801247889995575\ttrain disc cost\t-0.4945021867752075\tjs1\t0.020733090341530695\tjs2\t0.10705874809706785\tjs3\t0.3990938009493087\tjs4\t0.7228453212176902\n",
      "iter 135999\ttime\t0.18177597880363464\ttrain disc cost\t-0.488709032535553\tjs1\t0.021610973494890964\tjs2\t0.10543239401387536\tjs3\t0.38802788752537587\tjs4\t0.7097025209253326\n",
      "iter 136999\ttime\t0.18014861750602723\ttrain disc cost\t-0.48287516832351685\tjs1\t0.01947211832031795\tjs2\t0.10547091024851059\tjs3\t0.40033315334648845\tjs4\t0.7180130891914844\n",
      "iter 137999\ttime\t0.18054540872573852\ttrain disc cost\t-0.47990700602531433\tjs1\t0.020839892840309147\tjs2\t0.10929088461698512\tjs3\t0.4023388295212501\tjs4\t0.732917441367545\n",
      "iter 138999\ttime\t0.1800494978427887\ttrain disc cost\t-0.5004212856292725\tjs1\t0.02099085559357893\tjs2\t0.10781381917889256\tjs3\t0.4057005485418761\tjs4\t0.7269520979649524\n",
      "iter 139999\ttime\t0.1801961097717285\ttrain disc cost\t-0.4953819513320923\tjs1\t0.024998470226659904\tjs2\t0.11706554620797127\tjs3\t0.4062260534130388\tjs4\t0.7174152329872436\n",
      "iter 140999\ttime\t0.18013714146614074\ttrain disc cost\t-0.477776437997818\tjs1\t0.023428028719583747\tjs2\t0.10659245873282325\tjs3\t0.39613298908718025\tjs4\t0.7162791406949847\n",
      "iter 141999\ttime\t0.18012465977668762\ttrain disc cost\t-0.49066075682640076\tjs1\t0.020810961473880136\tjs2\t0.10349491552813773\tjs3\t0.4046216961393646\tjs4\t0.7275714862429041\n",
      "iter 142999\ttime\t0.18027992916107177\ttrain disc cost\t-0.48286592960357666\tjs1\t0.020727548959871372\tjs2\t0.10107202614097627\tjs3\t0.39196393512633404\tjs4\t0.7129492600954955\n",
      "iter 143999\ttime\t0.18023591709136963\ttrain disc cost\t-0.48047327995300293\tjs1\t0.0213379628029963\tjs2\t0.10887359064887946\tjs3\t0.41192757333723806\tjs4\t0.7235486168447152\n",
      "iter 144999\ttime\t0.18026160049438478\ttrain disc cost\t-0.49218106269836426\tjs1\t0.020426415813732585\tjs2\t0.1026149324166415\tjs3\t0.396271766149418\tjs4\t0.7178620054971449\n",
      "iter 145999\ttime\t0.17994712901115417\ttrain disc cost\t-0.4940200448036194\tjs1\t0.019822628118343216\tjs2\t0.10471793455709011\tjs3\t0.3982937538503959\tjs4\t0.7206270668708363\n",
      "iter 146999\ttime\t0.17981371426582338\ttrain disc cost\t-0.48509112000465393\tjs1\t0.018833051563087382\tjs2\t0.10305923587542547\tjs3\t0.394490268576681\tjs4\t0.7146292114989359\n",
      "iter 147999\ttime\t0.1792852623462677\ttrain disc cost\t-0.49440592527389526\tjs1\t0.019527046499746382\tjs2\t0.10232169061428262\tjs3\t0.38750635585438936\tjs4\t0.718343599059287\n",
      "iter 148999\ttime\t0.18083242917060852\ttrain disc cost\t-0.4830440282821655\tjs1\t0.021587051036850997\tjs2\t0.10564587103493762\tjs3\t0.3893686878134403\tjs4\t0.7089562864657623\n",
      "iter 149999\ttime\t0.17919901776313782\ttrain disc cost\t-0.48364540934562683\tjs1\t0.02034739078863446\tjs2\t0.10297483349448579\tjs3\t0.3970592848649097\tjs4\t0.7131427605479853\n",
      "iter 150999\ttime\t0.1793715636730194\ttrain disc cost\t-0.47856035828590393\tjs1\t0.021558231920687527\tjs2\t0.10717373747240916\tjs3\t0.40230099036927414\tjs4\t0.7302531285599527\n",
      "iter 151999\ttime\t0.17925833868980406\ttrain disc cost\t-0.47808772325515747\tjs1\t0.021202505533883777\tjs2\t0.1092329982077357\tjs3\t0.39809460441855005\tjs4\t0.716333435910044\n",
      "iter 152999\ttime\t0.17944152474403383\ttrain disc cost\t-0.4856651723384857\tjs1\t0.018219602056385533\tjs2\t0.09666160557307572\tjs3\t0.39327357621505166\tjs4\t0.7147169404053637\n",
      "iter 153999\ttime\t0.17942953658103944\ttrain disc cost\t-0.4772716462612152\tjs1\t0.022173321907848563\tjs2\t0.10980801127161312\tjs3\t0.4089530966828509\tjs4\t0.7260385720744515\n",
      "iter 154999\ttime\t0.17918615651130676\ttrain disc cost\t-0.4862377345561981\tjs1\t0.021160577258619123\tjs2\t0.11041397887706342\tjs3\t0.41407813941920285\tjs4\t0.7348192557960449\n",
      "iter 155999\ttime\t0.17979953289031983\ttrain disc cost\t-0.48517653346061707\tjs1\t0.025615219359874437\tjs2\t0.10927472363411196\tjs3\t0.4005375272403982\tjs4\t0.7191887868565997\n",
      "iter 156999\ttime\t0.17954549670219422\ttrain disc cost\t-0.4879379868507385\tjs1\t0.017700417351117735\tjs2\t0.10111026750681372\tjs3\t0.3945821756220435\tjs4\t0.716335042815869\n",
      "iter 157999\ttime\t0.17948873353004455\ttrain disc cost\t-0.4839228391647339\tjs1\t0.018565956335224343\tjs2\t0.1006142519255234\tjs3\t0.3848446766027718\tjs4\t0.7062416007058537\n",
      "iter 158999\ttime\t0.1793801462650299\ttrain disc cost\t-0.49092528223991394\tjs1\t0.02026202402450031\tjs2\t0.10631337544304485\tjs3\t0.4000774132274513\tjs4\t0.7250045219226099\n",
      "iter 159999\ttime\t0.17922559213638306\ttrain disc cost\t-0.48187437653541565\tjs1\t0.018192008696298025\tjs2\t0.09773842593168361\tjs3\t0.39602107556692895\tjs4\t0.7218056688431401\n",
      "iter 160999\ttime\t0.18057614707946779\ttrain disc cost\t-0.47986656427383423\tjs1\t0.021253525966221976\tjs2\t0.10427824708857673\tjs3\t0.4000529122103113\tjs4\t0.7157815786807404\n",
      "iter 161999\ttime\t0.17928626441955567\ttrain disc cost\t-0.4810407757759094\tjs1\t0.019608217830951267\tjs2\t0.10330129487557153\tjs3\t0.39435754795175215\tjs4\t0.7176794249231991\n",
      "iter 162999\ttime\t0.17931995892524719\ttrain disc cost\t-0.4809723496437073\tjs1\t0.019390521595658627\tjs2\t0.10396226815800058\tjs3\t0.3948514846315357\tjs4\t0.7119569940414789\n",
      "iter 163999\ttime\t0.17941519856452942\ttrain disc cost\t-0.47621527314186096\tjs1\t0.019673463901564568\tjs2\t0.0986597238229438\tjs3\t0.3926980853901944\tjs4\t0.7210876837065\n",
      "iter 164999\ttime\t0.17913096594810485\ttrain disc cost\t-0.4848572611808777\tjs1\t0.02095023942040762\tjs2\t0.10366221565489211\tjs3\t0.39567964841807857\tjs4\t0.7160492074604431\n",
      "iter 165999\ttime\t0.17926829051971435\ttrain disc cost\t-0.48008301854133606\tjs1\t0.020080084901434328\tjs2\t0.1026078915429088\tjs3\t0.4079400703876677\tjs4\t0.7216531956113152\n",
      "iter 166999\ttime\t0.1793142328262329\ttrain disc cost\t-0.47283509373664856\tjs1\t0.019600083263502537\tjs2\t0.1032577229662995\tjs3\t0.4012534207460006\tjs4\t0.7169582647163465\n",
      "iter 167999\ttime\t0.1791572422981262\ttrain disc cost\t-0.47783562541007996\tjs1\t0.018716309976539638\tjs2\t0.10034578446197462\tjs3\t0.39590651233806357\tjs4\t0.7207482064300837\n",
      "iter 168999\ttime\t0.179348717212677\ttrain disc cost\t-0.4796909987926483\tjs1\t0.019561480864257914\tjs2\t0.10424510765529796\tjs3\t0.3939354937446587\tjs4\t0.7203029596999653\n",
      "iter 169999\ttime\t0.17913597512245177\ttrain disc cost\t-0.4696682095527649\tjs1\t0.021043860779865194\tjs2\t0.10742132566734483\tjs3\t0.3992532707606191\tjs4\t0.7222302029155871\n",
      "iter 170999\ttime\t0.17912767338752747\ttrain disc cost\t-0.4741024076938629\tjs1\t0.023407356325201523\tjs2\t0.10904832224885708\tjs3\t0.409345547872292\tjs4\t0.7298971665765491\n",
      "iter 171999\ttime\t0.1792676923274994\ttrain disc cost\t-0.48177582025527954\tjs1\t0.019722400926539315\tjs2\t0.1058440287489393\tjs3\t0.3946439334729892\tjs4\t0.7209384442258894\n",
      "iter 172999\ttime\t0.1791897692680359\ttrain disc cost\t-0.4767778515815735\tjs1\t0.01988402776787467\tjs2\t0.09778295325444478\tjs3\t0.3901509363710496\tjs4\t0.7119623856167703\n",
      "iter 173999\ttime\t0.18072951173782348\ttrain disc cost\t-0.46463751792907715\tjs1\t0.02270127662797932\tjs2\t0.10127211842345449\tjs3\t0.3873423122601143\tjs4\t0.7073362190211149\n",
      "iter 174999\ttime\t0.17927845001220702\ttrain disc cost\t-0.4720169007778168\tjs1\t0.018620660356815373\tjs2\t0.10118793894248251\tjs3\t0.3831598890839903\tjs4\t0.6995923470670711\n",
      "iter 175999\ttime\t0.17922217512130736\ttrain disc cost\t-0.4785338044166565\tjs1\t0.020066997530334363\tjs2\t0.10492327268819385\tjs3\t0.389428986835242\tjs4\t0.7199907011894315\n",
      "iter 176999\ttime\t0.17929030871391297\ttrain disc cost\t-0.4738990366458893\tjs1\t0.018719713932204746\tjs2\t0.09959487910749779\tjs3\t0.3933187331116431\tjs4\t0.7177788315506167\n",
      "iter 177999\ttime\t0.1791908564567566\ttrain disc cost\t-0.4716949760913849\tjs1\t0.016332343822242707\tjs2\t0.10120276934042507\tjs3\t0.3946419362076659\tjs4\t0.7197198208551885\n",
      "iter 178999\ttime\t0.17922854828834534\ttrain disc cost\t-0.4560270309448242\tjs1\t0.017434107838441192\tjs2\t0.096608755376202\tjs3\t0.3948792180216711\tjs4\t0.7201709887538914\n",
      "iter 179999\ttime\t0.17907792568206787\ttrain disc cost\t-0.46055296063423157\tjs1\t0.01765908035057961\tjs2\t0.0997031922552071\tjs3\t0.40035244897191963\tjs4\t0.7203694803612727\n",
      "iter 180999\ttime\t0.17926204895973205\ttrain disc cost\t-0.4638853967189789\tjs1\t0.018711439438357708\tjs2\t0.09751609598004118\tjs3\t0.39131553771063005\tjs4\t0.7107902884523252\n",
      "iter 181999\ttime\t0.17941425013542175\ttrain disc cost\t-0.4551182985305786\tjs1\t0.016666875902158606\tjs2\t0.09790966008123643\tjs3\t0.3853565559922751\tjs4\t0.7210926559134853\n",
      "iter 182999\ttime\t0.17946103286743165\ttrain disc cost\t-0.45226991176605225\tjs1\t0.015926498881907426\tjs2\t0.09852259877228707\tjs3\t0.39566545675481807\tjs4\t0.7180216392491436\n",
      "iter 183999\ttime\t0.17929968690872192\ttrain disc cost\t-0.46396660804748535\tjs1\t0.016990326099949108\tjs2\t0.10319812320135081\tjs3\t0.39536757139515416\tjs4\t0.7134490150194419\n",
      "iter 184999\ttime\t0.17904261541366578\ttrain disc cost\t-0.4617181718349457\tjs1\t0.017169681509500694\tjs2\t0.10329604047531009\tjs3\t0.4020795852247565\tjs4\t0.726672520936455\n",
      "iter 185999\ttime\t0.18046588015556336\ttrain disc cost\t-0.4655452370643616\tjs1\t0.020432965668232664\tjs2\t0.10337555792649476\tjs3\t0.3936256665897135\tjs4\t0.7142463491659995\n",
      "iter 186999\ttime\t0.17940053820610047\ttrain disc cost\t-0.4605109691619873\tjs1\t0.01821188881426645\tjs2\t0.10055430496462732\tjs3\t0.39620749427376967\tjs4\t0.7155121712529318\n",
      "iter 187999\ttime\t0.1791038429737091\ttrain disc cost\t-0.4550042748451233\tjs1\t0.018195784573776234\tjs2\t0.10331421247330642\tjs3\t0.39273101373466496\tjs4\t0.7136875257713841\n",
      "iter 188999\ttime\t0.1793564944267273\ttrain disc cost\t-0.4545050859451294\tjs1\t0.017240543150408433\tjs2\t0.10285540297370294\tjs3\t0.3985649572107236\tjs4\t0.7253128162440678\n",
      "iter 189999\ttime\t0.17914590430259705\ttrain disc cost\t-0.4581815302371979\tjs1\t0.0191865624857772\tjs2\t0.10275157020492351\tjs3\t0.3926260653589147\tjs4\t0.7151292717698279\n",
      "iter 190999\ttime\t0.17943176817893983\ttrain disc cost\t-0.4561004042625427\tjs1\t0.018515399416324168\tjs2\t0.1002181613316139\tjs3\t0.3814711016660013\tjs4\t0.7004973282562116\n",
      "iter 191999\ttime\t0.17930993890762328\ttrain disc cost\t-0.4569658935070038\tjs1\t0.01781199777822139\tjs2\t0.10219607253468614\tjs3\t0.40058874426971686\tjs4\t0.713741901617613\n",
      "iter 192999\ttime\t0.17946990966796875\ttrain disc cost\t-0.4629587233066559\tjs1\t0.020625677131363693\tjs2\t0.11044711393885293\tjs3\t0.3969644927644744\tjs4\t0.7194619030566762\n",
      "iter 193999\ttime\t0.17962329244613648\ttrain disc cost\t-0.46077466011047363\tjs1\t0.020570576777131797\tjs2\t0.10408331888014677\tjs3\t0.4025094268126022\tjs4\t0.7271202442235073\n",
      "iter 194999\ttime\t0.17928269982337952\ttrain disc cost\t-0.45760348439216614\tjs1\t0.018014672060946407\tjs2\t0.10101941528741576\tjs3\t0.3875626348120421\tjs4\t0.7138293330330887\n",
      "iter 195999\ttime\t0.17926961898803712\ttrain disc cost\t-0.45928099751472473\tjs1\t0.01831547039307704\tjs2\t0.094530534826083\tjs3\t0.38603526059557974\tjs4\t0.7198101223703071\n",
      "iter 196999\ttime\t0.17967708015441894\ttrain disc cost\t-0.45338860154151917\tjs1\t0.021133313269105464\tjs2\t0.11400315744428959\tjs3\t0.40655750700311377\tjs4\t0.7238346794806713\n",
      "iter 197999\ttime\t0.1803571207523346\ttrain disc cost\t-0.4574245512485504\tjs1\t0.01625442428993599\tjs2\t0.09298346701820964\tjs3\t0.385608427062638\tjs4\t0.716644388563519\n",
      "iter 198999\ttime\t0.17928283381462098\ttrain disc cost\t-0.45953065156936646\tjs1\t0.015917722867204467\tjs2\t0.09768941460580179\tjs3\t0.3789990681284294\tjs4\t0.7072234188299437\n",
      "iter 199999\ttime\t0.17907736015319825\ttrain disc cost\t-0.4594208896160126\tjs1\t0.01864908627390081\tjs2\t0.0958186944554812\tjs3\t0.3837688347624717\tjs4\t0.7097648979044517\n"
     ]
    }
   ],
   "source": [
    "checkpoint_iters = [5000, 15000, 25000, 35000, 45000, 55000, 65000, 75000, 85000, 95000, 105000, 115000, 125000, 135000, 145000, 155000, 165000, 175000, 185000, 195000, 199000]\n",
    "\n",
    "with tf.compat.v1.Session() as session:\n",
    "    session.run(tf.compat.v1.initialize_all_variables())\n",
    "    \n",
    "    saver = tf.compat.v1.train.Saver(max_to_keep=100)\n",
    "\n",
    "    def generate_samples():\n",
    "        samples = session.run(fake_inputs)\n",
    "        samples = np.argmax(samples, axis=2)\n",
    "        decoded_samples = []\n",
    "        for i in range(len(samples)):\n",
    "            decoded = []\n",
    "            for j in range(len(samples[i])):\n",
    "                decoded.append(inv_charmap[samples[i][j]])\n",
    "            decoded_samples.append(tuple(decoded))\n",
    "        return decoded_samples\n",
    "\n",
    "    gen = inf_train_gen()\n",
    "\n",
    "    for iteration in range(ITERS):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train generator\n",
    "        if iteration > 0:\n",
    "            _ = session.run(gen_train_op)\n",
    "\n",
    "        # Train critic\n",
    "        for i in range(CRITIC_ITERS):\n",
    "            _data = next(gen)\n",
    "            _disc_cost, _ = session.run(\n",
    "                [disc_cost, disc_train_op],\n",
    "                feed_dict={real_inputs_discrete:_data}\n",
    "            )\n",
    "\n",
    "        plot('time', time.time() - start_time)\n",
    "        plot('train disc cost', _disc_cost)\n",
    "\n",
    "        if iteration % 1000 == 999:\n",
    "            samples = []\n",
    "            for i in range(CRITIC_ITERS):\n",
    "                samples.extend(generate_samples())\n",
    "\n",
    "            for i in range(4):\n",
    "                lm = NgramLanguageModel(i+1, samples, tokenize=False)\n",
    "                plot('js{}'.format(i+1), lm.js_with(true_char_ngram_lms[i]))\n",
    "\n",
    "            with open('samples_{}.txt'.format(iteration), 'w') as f:\n",
    "                for s in samples:\n",
    "                    s = \"\".join(s)\n",
    "                    f.write(s + \"\\n\")\n",
    "                    \n",
    "        #if iteration % 5000 == 0:\n",
    "        if iteration in checkpoint_iters:\n",
    "            saver.save(session, save_path='model_e'+str(iteration))\n",
    "\n",
    "        if iteration % 1000 == 999:\n",
    "            flush()\n",
    "        \n",
    "        tick()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b1e470",
   "metadata": {
    "papermill": {
     "duration": 0.020357,
     "end_time": "2023-05-14T15:43:55.582477",
     "exception": false,
     "start_time": "2023-05-14T15:43:55.562120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 39687.318686,
   "end_time": "2023-05-14T15:43:58.516830",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-05-14T04:42:31.198144",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
